![UTA-DataScience-Logo](https://github.com/user-attachments/assets/6d626bcc-5430-4356-927b-97764939109d)

# Finance PDF Q&A Model
* This project builds an end-to-end LLM-powered system that answers questions about financial reports (PDFs). Users can either **auto-generate questions** from the uploaded document or **manually type their own**, then receive context-aware answers based on document retrieval.

* To assess answer quality, the system includes an **LLM-based evaluation module** that scores each response across three dimensions: **factual correctness**, **completeness**, and **clarity**. These scores are generated entirely by the language model using a structured evaluation rubric.

* **Key Components**:
  * **LangChain** – framework for managing chunking, document embedding, retrieval, and chaining components together
  * **Perplexity's Sonar LLM** – used for question generation, answer generation, and evaluation
  * **FAISS** – enables fast vector similarity search over document chunks
  * **Streamlit** – provides an interactive web app for user input and result visualization
  * **Plotly** – used to generate evaluation metric visualizations (box plots, histograms, heatmaps)

## Overview
* Financial documents are often dense, multi-page, and highly structured. Extracting useful insights through LLMs presents challenges in chunking, hallucination prevention, and evaluation.

* This project simulates a realistic pipeline where:
  * Annual reports are broken into chunks and summarized
  * Questions are auto-generated using **Perplexity Sonar**
  * A retrieval-based QA model answers these questions
  * Each answer is scored by the LLM evaluator using a 5-point rubric on Factual Correctness, Completeness, and Clarity

## Repository Structure
```sh
├── app.py                     # Streamlit web app for uploading and interacting with PDFs
├── config.py                  # Project config and global constants
├── data_processing.py         # PDF loading, chunking, and summarization
├── question_generator.py      # LLM-based question generation from chunks
├── retriever_model.py         # Embedding, FAISS index creation, retrieval pipeline
├── evaluation.py              # Auto-evaluation using rubric-based prompts
├── visualize.py               # Plotly visualizations for evaluation metrics
├── main.ipynb                 # End-to-end notebook running the full pipeline
├── annual_report/             # Sample input folder containing financial PDFs
├── session_1/                 # Output folder: metadata, questions, answers, evaluated CSVs
├── faiss_index_open/          # Saved FAISS vector store
```
## Summary of Workdone
### Data
* **Input**:
  * PDF financial reports from 10 different companies placed in ```annual_report/```
  * Each file is processed independently

* **Output**:
  * Summary of first 3 pages
  * List of chunked text segments (JSON)
  * Auto-generated questions (```questions.csv```)
  * Model-generated answers (```answers.csv```)
  * Evaluation scores and comments (```evaluated.csv```)

### Preprocessing
* **PDF Loading**: Used ```PyMuPDFLoader``` via LangChain to extract full page text
* **Summarization**: Queried Perplexity Sonar to summarize first 3 pages (single-sentence summary of company and year)
* **Chunking**: Used ```RecursiveCharacterTextSplitter``` to create overlapping 5,000-character chunks with 100-character overlap
* **Metadata Tracking**: Each file's word count, chunk count, and summary are saved to ```metadata.json```

### Auto-Generate Question
Each question is generated by:
  * Randomly selecting a chunk from the document
    
  * Combining it with the document summary
    
  * Prompting Sonar with detailed instructions to:
    * Ground the question only in the chunk
    * Use company/year context from the summary
    * Avoid adding information not present in the chunk

  * If the chunk is irrelevant or noisy, the model returns an empty string

Questions are saved to ```questions.csv``` with metadata like chunk ID and source file.

### Model Architecture
Retrieval QA Stack:
* **Embeddings**: all-mpnet-base-v2 via HuggingFaceEmbeddings
* **Vector Store**: FAISS index for all document chunks
* **Retriever**: Top-k = 3 nearest neighbor chunks
* **LLM**: Perplexity Sonar (accessed via OpenAI-compatible client)
* **QA Chain**: LangChain RetrievalQA with stuff mode

### Evaluation Method
* Each answer is evaluated using a custom prompt fed to **Perplexity Sonar**, scoring across
  
| Metric              | Scale (1–5)                | Description                  |
| ------------------- | -------------------------- | ---------------------------- |
| Factual Correctness | 1 (Wrong) to 5 (Accurate)  | Matches facts from chunk     |
| Completeness        | 1 (Incomplete) to 5 (Full) | Covers all parts of question |
| Clarity             | 1 (Confusing) to 5 (Clear) | Easy to read and understand  |

### Overall Performance & Visualization
* **Histogram**: Overall score distribution with mean line
  <img width="1263" height="386" alt="image" src="https://github.com/user-attachments/assets/a8f43cc7-b432-4134-a4ec-7df689b6af34" />
The model shows high factual reliability and excellent clarity, with 8 out of 10 responses scoring a perfect 5
  
* **Summary Table**: Evaluation metrics for **Factual Correctness**, **Completeness**, **Clarity**, and **Overall Score**
<img width="776" height="249" alt="image" src="https://github.com/user-attachments/assets/2f0b2e93-14ca-4d84-be6c-38581989aa89" />

## Reproduce the Results
**Clone the Repo**
  ```sh
  git clone https://github.com/Landaoduy/Finance-PDF-QA-Model.git
  cd Finance-PDF-QA-Model
  ```
**Install dependencies**
  ```sh
  pip install -r requirements.txt
  ```
  Or manually install:
  ```sh
  pip install langchain openai pymupdf faiss-cpu sentence-transformers pandas plotly streamlit
  ```
**Set your API Key**
* To use the Perplexity Sonar model, you'll need an API key:
  1. Go to https://www.perplexity.ai/api
  2. Sign in and click "Create API Key"
  3. Copy the key (starts with sk-...)

Then, update the config.py file in the project root:
```sh
# config.py
API_KEY = "sk-..."  # Paste your API key here
```
**Add your PDFs**
* Drop any financial reports into the annual_report/ folder.

**Run the full pipeline**
* Open ```main.ipynb``` and run all cells to:
  1. Chunk and summarize the PDFs
  2. Generate and save questions
  3. Build retriever and answer questions
  4. Evaluate and visualize performance

## Run the Streamlit App (Optional)
```sh
streamlit run app.py
```
You can:
* Upload a new PDF
* Auto-generate or type your question
* Get an answer and evaluate it live
* See your Q&A history in the sidebar
<img width="1881" height="976" alt="image" src="https://github.com/user-attachments/assets/2e293049-b09c-48ee-9af6-87b2648b6de9" />


